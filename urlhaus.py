import requests
import hashlib
import random
import os

proxies = {'https': '10.28.143.3:7890'}

def get_webcontent(url):

    response = requests.get(url,proxies=proxies)
    if response.status_code == 200:
        
        return response.content
    else:
        raise Exception("Fail to get URL!")
        return False
        
def get_md5_of_file(filename):

    md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5.update(chunk)
    return md5.hexdigest()
  
def download_from_url(url,save_file):
    
    try:
        response = requests.get(url,proxies=proxies,stream=True,timeout=8)
        if response.status_code == 200:
            with open(save_file, "wb") as f:
                for ch in response:            
                    f.write(ch)
                f.close() 
                
            md5 = get_md5_of_file(save_file)
            print(md5)
            new_filename = './sample/' + md5
            if not os.path.exists(new_filename):
                os.rename(save_file, new_filename)
            else:
                os.remove(save_file)               
    except requests.exceptions.RequestException as e:
        print("Error downloading file:"+save_file, e)


def deal_files(path):
    for filename in os.listdir(path):
        filepath = os.path.join(path, filename)
        if os.path.getsize(filepath) < 2048:
            os.remove(filepath)
        if len(filename)!= 32:
            print(filepath)
            md5 = get_md5_of_file(filepath)
            new_filename = path + md5
            os.rename(filepath, new_filename)
       
        
if __name__ == "__main__":

    if not os.path.exists('sample'):
        os.makedirs('sample')
            
    webcontent= get_webcontent('https://urlhaus.abuse.ch/downloads/text_online/')   
    urllist = str(webcontent).split('http')
    
    n = 0
    
    for url in urllist:
        if n>0:
            url = 'http' + url.split('\\')[0]
            print(n)

            print(url)
            name = str(random.randint(1, 10000))
            save_file = os.getcwd() + '/sample/' + name

            if not os.path.exists(save_file):
                download_from_url(url,save_file)
            print('———————————————————————————————————————') 
        n = n + 1

    deal_files('./sample/')    
    